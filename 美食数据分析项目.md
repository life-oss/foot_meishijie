# 美食数据分析

## Ⅰ.项目结构

代码已放置[GitHub](https://github.com/life-oss/foot_meishijie.git)上

~~~properties
─foot_meishijie
│  app.py
│
├─.idea
│  │  .gitignore
│  │  foot_meishijie.iml
│  │  misc.xml
│  │  modules.xml
│  │  vcs.xml
│  │  workspace.xml
│  │
│  └─inspectionProfiles
│          profiles_settings.xml
│
├─get_caipu
│      analyze_url.py
│      get_url.py
│      get_data.py
│      test.py
│
├─static
│  │  美食数据分析.docx
│  │  美食数据分析.md
│  │  美食数据分析.pdf
│  │
│  ├─css
│  │      cyclopedia.css
│  │      font-awesome.min.css
│  │      header.css
│  │      index.css
│  │      reset.css
│  │      show.css
│  │      swiper-bundle.min.css
│  │
│  ├─img
│  │  │  宫保鸡丁.jpg
│  │  │  水煮肉片.jpg
│  │  │  酱爆鸡丁.jpg
│  │  │
│  │  └─caixi
│  │          川菜.jpg
│  │          徽菜.jpg
│  │          浙菜.jpg
│  │          湘菜.jpg
│  │          粤菜.jpg
│  │          苏菜.jpg
│  │          闽菜.jpg
│  │          鲁菜.jpg
│  │
│  └─js
│          autoload.js
│          echarts.min.js
│          jquery-ui.js
│          jquery.js
│          marked.min.js
│          swiper-bundle.min.js
│
├─templates
│  │  about.html
│  │  cyclopedia.html
│  │  index.html
│  │  show.html
│  │
│  ├─echarts
│  │      bar.html
│  │      pie.html
│  │
│  └─wordcount
│          ingredient1.html
│          ingredient2.html
│
└─visual
        bar_make.py
        pie_make.py
        get_visual.py
        stop_word.txt
        test.py
        wordcount.py
~~~

## Ⅱ.项目技术

~~~properties
爬虫
requests拿取+xpath解析+re清洗+mysql存储
分析
pyecharts制作+mysql分析
可视化
html+css+javascript+flask+博客园小玩偶+swiper轮播图
~~~

## Ⅲ.运行前提

~~~properties
MySQL表中需要拥有对应两张表和MySQL配置
~~~

## Ⅳ.运行过程

~~~properties
三步运行
①运行项目文件夹下get_caipu文件夹下的get_data.py文件拿数据
②运行项目文件夹下visual文件夹下的get_visual.py文件生成所有图表加上分析
③运行项目文件夹下app.py文件，进行可视化展示
~~~

## Ⅴ.实施过程

### ①.数据获取

#### 思路阐述

~~~properties
1.拿链接地址存放
想法：先拿到所有的url链接
原因：拿到菜系字段，还需要拿到菜系url，获取其中的菜系字段
实施：使用request请求和xpath提取链接，存放mysql表中
2.拿数据存放
想法：拿所有数据
原因：有些数据不够完美
实施：使用xpath拿到想要的数据，然后使用re清洗部分数据
~~~

#### **代码实施**

#### 1.拿到链接地址存放

##### MySQL建表：

~~~mysql
CREATE TABLE `url` (
	`id` INT NOT NULL AUTO_INCREMENT,
	`dishes_system` VARCHAR ( 255 ),
	`url` VARCHAR ( 255 ),
PRIMARY KEY ( `id` ) 
)
~~~

##### 代码：

~~~python
import re

import pymysql
import requests
from lxml import etree

"""此程序用于拿取需要爬取的所有url，存于MySQL数据库当中"""

'''
第一步：
爬取网页需要用到的，需要执行多次
'''


def get_url(url):
    # 步骤：
    # 1.模拟浏览器头部信息
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) "
                      "Chrome/91.0.4455.2 Safari/537.36 "
    }
    cookie = {
        'Cookie': 'MSCookieKey=c2c74a02aa8c4d493a717aab3c849a40.; UM_distinctid=17922cf98f0a12-0edd0fab18db9-6e52772c-144000-17922cf98f1f8c; CNZZDATA1259001544=240567737-1619786531-https%253A%252F%252Fso.meishi.cc%252F%7C1619786531; Hm_lvt_01dd6a7c493607e115255b7e72de5f40=1620391059,1620431749,1620446522,1620446626; Hm_lpvt_01dd6a7c493607e115255b7e72de5f40=1620446633'}
    # 2.请求,返回页面数据
    try:
        response = requests.get(url, headers=headers, cookies=cookie)
        # 针对于重定向网址
        if response.url == url:
            html = response.text
            return html
    except requests.exceptions.ConnectionError:
        requests.status_code = "Connection refused"


'''
第二步：
获取中华类别的所有菜系链接
category:是每一大类别的超链接，也是首页
'''


def get_category():
    # 步骤
    # 1.运行get_url方法，拿到最初网址https://www.meishij.net/caipufenlei/
    html = get_url("https://www.meishij.net/caipufenlei/")
    # 2.使用xpath定位元素
    list = etree.HTML(html)
    category_url = list.xpath('/html/body/div[2]/div[1]/div[5]/ul/li/a/@href')
    # 3.循环
    for i in range(len(category_url)):
        url = 'https://www.meishij.net' + str(category_url[i]) + '?order=-fav_num'
        # 4.将列表中不完整的链接加入
        category_url[i] = url
        i += 1
        # 5.输出查看（已经注释）
        # print(url)
    # print(category_url)
    # 6.返回菜谱分类列表
    return category_url


'''
第三步：详情页
'''

# 用于存放其中一系列所有的详情页地址
all_url = []


def detail_page(html):
    list = etree.HTML(html)
    # 详情页的地址
    detail = list.xpath('//div[@class="list_s2_item"]/div/a[1]/@href')
    for i in range(len(detail)):
        all_url.append(detail[i])
        i += 1


'''
第四步：翻页
获取最后的详情页
'''


def next_page(url):
    next_url = [url]
    # 解析这个网页
    # html=get_url(url)
    for i in range(2, 10):
        urls = re.sub('\?order=-fav_num', '', url)
        url = urls + f'p{i}/?order=-fav_num'
        next_url.append(url)
    # 清空
    # all_url = []
    for j in next_url:
        html = get_url(j)
        detail_page(html)
    return all_url


'''
保存数据
'''


def set_data():
    # 获取菜谱系别
    dishes_system_list = get_category()
    # print(dishes_system_list)
    # 获取菜谱详情
    # url_list = next_page()
    db = pymysql.connect(host='localhost', user='root', passwd='123456', db='foot', port=3306, charset='utf8')
    cursor = db.cursor()
    for i in dishes_system_list:
        # 首先知道每一大菜系，以每一大菜系做循环
        dishes_system = i
        # print(dishes_system)
        # 列表
        # url_list=next_page(dishes_system)
        # 以每一系列作为
        all_url.clear()
        url_list = next_page(dishes_system)
        # print(url_list)
        for j in range(len(url_list)):
            url = url_list[j]
            # print(url + "第" + str(j) + "条")
            sql = 'insert into foot_url values(0,\"%s\",\"%s\")' % (str(dishes_system), str(url))
            cursor.execute(sql)
            db.commit()
    print('爬取完成，恭喜恭喜！')
    cursor.close()
    db.close()


if __name__ == '__main__':
    # 测试详情链接的数据
    # print(next_page('https://www.meishij.net/fenlei/jiangxicai/?order=-fav_num'))
    # print(all_page())
    # 测试菜系的数据
    # print(get_category())
    # 运行保存数据
  	set_data()
    # url = get_category()
    # for i in url:
    #     a, b = next_page(i)
    #     print(a)
    #     print(b)
~~~

#### 2.拿数据存放

##### MySQL建表：

~~~mysql
CREATE TABLE `menu` (
  `id` int NOT NULL AUTO_INCREMENT,
  `dishes_system` varchar(255) COMMENT '菜系',
  `dish_name` varchar(255) COMMENT '菜名',
  `private_int_fcount` bigint DEFAULT NULL COMMENT '收藏数',
  `main_ingredient` varchar(255) COMMENT '主料',
  `ingredient` varchar(255) COMMENT '辅料',
  PRIMARY KEY (`id`)
)
~~~

##### 代码：

~~~python
import re

import pymysql
from lxml import etree

from get_url import get_url

"""此程序通过get_url所拿到的地址获取需要获取的信息"""


def get_data():
    db = pymysql.connect(host='localhost', user='root', passwd='123456', db='foot', port=3306, charset='utf8')
    cursor = db.cursor()
    sql = 'select dishes_system,url from foot_url;'
    cursor.execute(sql)
    # 拿全部数据
    urls = cursor.fetchall()
    # print(url)
    i = 1

    for dishes_system_list, url in urls:
        dishes_system = method_name(str(dishes_system_list))
        dish_name, private_int_fcount, main_ingredient, ingredient = analyze(str(url))
        sql1 = 'insert into menu values(0,\"%s",\"%s",\"%s",\"%s",\"%s")' % (
            str(dishes_system), str(dish_name), str(private_int_fcount),
            str(main_ingredient), str(ingredient))
        cursor.execute(sql1)
        db.commit()
        print("写入第" + str(i) + "条数据")
        i += 1
    cursor.close()
    db.close()
    print("数据全部拿到，恭喜!!!")


def analyze(url):
    # 做出判断是否为重定向的地址
    # 然后拿到详情页的数据
    html = get_url(url)

    list = etree.HTML(html)
    # 菜名
    dish_name = \
        list.xpath('//*[@id="app"]/div[@class="recipe_header"]/div[1]/div[@class="recipe_header_info"]/h1/text()')[0]
    # 收藏数
    private_int_fcount = list.xpath('//*[@id="addfav_btn"]/em/text()')[0]
    # 主料
    main_ingredient_list = list.xpath(
        '//*[@id="app"]/div[@class="recipe_header"]/div[1]/div[@class="recipe_header_info"]/div[@class="recipe_ingredientsw"]/div[1]/div[2]/strong/a/text()')
    main_ingredient = ','.join(main_ingredient_list)
    # 辅料
    ingredient_list = list.xpath(
        '//*[@id="app"]/div[@class="recipe_header"]/div[1]/div[@class="recipe_header_info"]/div[@class="recipe_ingredientsw"]/div[2]/div[2]/strong/a/text()')
    ingredient = ','.join(ingredient_list)
    return dish_name, private_int_fcount, main_ingredient, ingredient


# 试验
# print(analyze('https://www.meishij.net/zuofa/chaoyixuetangcupaigu.html'))

def method_name(url):
    html = get_url(url)
    list = etree.HTML(html)
    dishes_system_list = list.xpath('//div[@class="list_s2"]/h1[@class="list_title"]/text()')[0]
    # 正则(.+?)菜
    dishes_system_match = re.match(r'(.+?)菜', str(dishes_system_list))
    dishes_system = dishes_system_match.group()
    return dishes_system

# 试验
# print(method_name('https://www.meishij.net/caixi/chuancai/'))
if __name__ == '__main__':
	get_data()
~~~

#### 3.整合

~~~properties
将两个文件中的方法整合至一个文件中
~~~

~~~python
from get_caipu.analyze_url import get_data
from get_caipu.get_url import set_data

"""
    运行顺序
    首先运行get_data.py将所有的数据存到数据库当中
    再运行app.py将所有效果展示
"""

if __name__ == '__main__':
    # 拿链接
    set_data()
    # 存数据
    get_data()
~~~

#### 4.执行

运行中

![image-20210526160821746](https://i.loli.net/2021/05/26/ZfIJAc3uohYCqea.png)

运行完成

![image-20210526165259512](https://i.loli.net/2021/05/26/zNQeYvFs7Glh64U.png)

提示：运行时间较长，是因为执行了多次循环

### 所遇问题：因为收藏数异常，所以改为浏览数

#### **未解决**

![image-20210526163240077](https://i.loli.net/2021/05/26/O5FrvotYCNpJ7Qs.png)

#### **已解决**

#### 原因

![image-20210526155430434](https://i.loli.net/2021/05/26/PN7AlYm1vnJa4jh.png)

#### 代码更改

新建表

~~~mysql
CREATE TABLE `new_menu` (
  `id` int NOT NULL AUTO_INCREMENT,
  `dishes_system` varchar(255) COMMENT '菜系',
  `dish_name` varchar(255) COMMENT '菜名',
  `browse_the_numbert` bigint DEFAULT NULL COMMENT '浏览数',
  `main_ingredient` varchar(255) COMMENT '主料',
  `ingredient` varchar(255) COMMENT '辅料',
  PRIMARY KEY (`id`)
)
~~~

analyze_url.py更改

~~~python
# 浏览数
    browse_the_number_list = list.xpath('//*[@id="app"]/div[@class="recipe_header"]/div[1]/div[@class="recipe_header_info"]/span[@class="info1"]/text()')[1]
    # 正则匹配
    # 先去掉空格和特殊字符
    browse_the_number_sub = re.sub('\s+|·', '', browse_the_number_list).strip()
    # 匹配浏览数
    browse_the_number_match = re.findall(r'(?<=藏).+?(?=浏)', str(browse_the_number_sub))
    browse_the_number = browse_the_number_match[0]
~~~

连接数据库的sql语句

~~~python
sql1 = 'insert into new_menu values(0,\"%s",\"%s",\"%s",\"%s",\"%s")' % (
            str(dishes_system), str(dish_name), str(browse_the_number),
            str(main_ingredient), str(ingredient))
~~~

### ②.数据分析

#### 思路阐述

~~~properties
分维度分析
1.求出热门菜系Top10
2.分析数据中各菜系的占比情况
3.通过主料和辅料生成词云图
使用pyecharts制作
~~~

#### 代码实施

##### 制作

柱形图制作

~~~python
import pymysql
from pyecharts.charts import Bar
from pyecharts import options as opts
from pyecharts.globals import ThemeType


def bar_make():
    db = pymysql.connect(host='localhost', user='root', passwd='123456', db='foot', port=3306, charset='utf8')
    cursor = db.cursor()
    sql = '''SELECT
        dishes_system,dish_name,
        browse_the_numbert
    FROM
        new_menu 
    ORDER BY
        browse_the_numbert DESC 
        LIMIT 0,
        10;'''
    cursor.execute(sql)
    # 拿全部数据
    urls = cursor.fetchall()
    # print(url)
    i = 1
    dish_name = []
    private_int_fcount = []
    for dish_name_list, private_int_fcount_list in urls:
        dish_name.append(dish_name_list)
        private_int_fcount.append(private_int_fcount_list)
    cursor.close()
    db.close()
    '''
    此程序用于柱形图可视化的创建
    '''
    c = (
        Bar(init_opts=opts.InitOpts(theme=ThemeType.LIGHT))
            .add_xaxis(dish_name)
            .add_yaxis('菜名', private_int_fcount)
            .set_global_opts(
            xaxis_opts=opts.AxisOpts(axislabel_opts=opts.LabelOpts(rotate=-15)),
            title_opts={"text": "中华菜系Top10", "subtext": "数据来源于美食杰"})
            .render("../templates/echarts/bar.html")
    )
~~~

饼图制作

~~~python
import pyecharts.options as opts
import pymysql
from pyecharts.charts import Pie
from pyecharts.globals import ThemeType


def get_data():
    db = pymysql.connect(host='localhost', user='root', passwd='123456', db='foot', port=3306, charset='utf8')
    cursor = db.cursor()
    sql = """SELECT DISTINCT
dishes_system,
	COUNT( dishes_system ) AS count 
FROM
	new_menu 
GROUP BY
	dishes_system 
ORDER BY
	count DESC;"""

    cursor.execute(sql)
    # 拿全部数据
    urls = cursor.fetchall()
    # print(url)
    dish_system = []
    count = []
    for dish_system_list, count_list in urls:
        dish_system.append(dish_system_list)
        count.append(count_list)
    cursor.close()
    db.close()
    return dish_system, count


def histogram():
    dish_name, count = get_data()
    x_data = dish_name
    y_data = count
    data_pair = [list(z) for z in zip(x_data, y_data)]
    data_pair.sort(key=lambda x: x[1])
    (
        Pie(init_opts=opts.InitOpts(bg_color='#717f8f', theme=ThemeType.LIGHT))
            .add(
            series_name="占比",
            data_pair=data_pair,
            rosetype="radius",
            radius="55%",
            center=["50%", "50%"],
            label_opts=opts.LabelOpts(is_show=False, position="center"),
        )
            .set_global_opts(
            title_opts=opts.TitleOpts(
                title="各菜系占比情况",
                pos_left="center",
                pos_top="20",
                title_textstyle_opts=opts.TextStyleOpts(color="#fff"),
            ),
            legend_opts=opts.LegendOpts(is_show=False),
        )
            .set_series_opts(
            tooltip_opts=opts.TooltipOpts(
                trigger="item", formatter="{a} <br/>{b}: {c} ({d}%)"
            ),
            label_opts=opts.LabelOpts(color="rgba(255, 255, 255, 0.3)"),
        )
            .render("../templates/echarts/pie.html")
    )


# if __name__ == '__main__':
#     histogram()
~~~

词云制作

~~~python
import collections
import jieba
import pymysql
import pyecharts.options as opts
from pyecharts.charts import WordCloud


def get_text(sql):
    db = pymysql.connect(host='localhost', user='root', passwd='123456', db='foot', port=3306, charset='utf8')
    cursor = db.cursor()
    cursor.execute(sql)
    # 拿全部数据
    urls = cursor.fetchall()
    text = ""
    for text_list in urls:
        text += text_list[0] + ""
    # print(text)
    return text


def split_word(text):
    word_list = list(jieba.cut(text, cut_all=False, HMM=True))
    with open("stop_word.txt", 'r', encoding='UTF-8') as meaninglessFile:
        stopwords = set(meaninglessFile.read().split('\n'))
    stopwords.add(' ')
    object_list = []
    for word in word_list:
        if word not in stopwords:
            object_list.append(word)
    # collections.Counter 计数器，统计单词个数
    word_counts = collections.Counter(object_list)
    word_count = word_counts.most_common(2000)
    # 筛选词语
    return word_count


def word_cloud(data, i, str):
    c = (
        WordCloud()
            .add(
            "",
            data,
            # 添加数据
            word_gap=5,
            word_size_range=[15, 80])
            .set_global_opts(
            title_opts=opts.TitleOpts(
                title=str, title_textstyle_opts=opts.TextStyleOpts(font_size=23)
            ),
            tooltip_opts=opts.TooltipOpts(is_show=True),
        )
            .render(f"../templates/wordcount/ingredient{i}.html")
    )


# if __name__ == '__main__':
#     # 主料词云分析
#     sql = 'select main_ingredient from foot_menu;'
#     data = split_word(get_text(sql))
#     word_cloud(data, 1, "主料分析")
#     # 配料词云分析
#     sql2 = 'select ingredient from foot_menu;'
#     data = split_word(get_text(sql2))
#     word_cloud(data, 2, "辅料分析")
~~~

##### 整合

~~~python
from visual.bar_make import bar_make
from visual.pie_make import histogram
from visual.wordcount import split_word, word_cloud, get_text

if __name__ == '__main__':
    # 柱形图运行
    bar_make()
    # 饼图运行
    histogram()
    # 词云运行
    # 主料词云分析
    sql = 'select main_ingredient from foot_menu;'
    data = split_word(get_text(sql))
    word_cloud(data, 1, "主料分析")
    # 配料词云分析
    sql2 = 'select ingredient from foot_menu;'
    data = split_word(get_text(sql2))
    word_cloud(data, 2, "辅料分析")
~~~

### ③.数据可视化展现

#### 思路阐述

~~~properties
通过flask框架进行离线展现，使用前端技术
~~~

#### 代码实施

代码放入static和templates文件夹中，运行app.py

~~~python
import flask
from flask import Flask

"""
    运行顺序
    首先运行get_data.py将所有的数据存到数据库当中
    再运行app.py将所有效果展示
"""

app = Flask(__name__)


# 前端页面
@app.route('/')
def index():
    return flask.render_template('index.html')


@app.route('/cyclopedia')
def cyclopedia():
    return flask.render_template('cyclopedia.html')


@app.route('/show')
def show():
    return flask.render_template('show.html')


@app.route('/about')
def about():
    return flask.render_template('about.html')


@app.route('/bar')
def bar():
    return flask.render_template('echarts/bar.html')


@app.route('/pie')
def pie():
    return flask.render_template('echarts/pie.html')


@app.route('/ingredient1')
def ingredient1():
    return flask.render_template('wordcount/ingredient1.html')


@app.route('/ingredient2')
def ingredient2():
    return flask.render_template('wordcount/ingredient2.html')


if __name__ == '__main__':
    app.run(host='127.0.0.1', port=8020)
~~~



## Ⅵ.分析报告

# 美食数据分析

## 需求背景介绍

中国是一个餐饮文化大国，长期以来在某一地区由于地理环境、气候物产、文化传统以及民族习俗等因素的影响，形成有一定亲缘承袭关系、菜点风味相近，知名度较高，并为部分群众喜爱的地方风味著名流派称作菜系。其中，鲁菜、川菜、粤菜、闽菜、苏菜、浙菜、湘菜、徽菜享称为\"八大菜系"。

在日常生活人们多热爱于哪些食物，哪些菜系？

## 数据说明

### 数据样例

![image-20210516185935824](https://i.loli.net/2021/05/16/zAyXLwC6BnSxjg8.png)

### 数据说明

| 数据说明      |           |                    |                 |            |
| ------------- | --------- | ------------------ | --------------- | ---------- |
| dishes_system | dish_name | private_int_fcount | main_infredient | infredient |
| 菜系          | 菜名      | 收藏数             | 主料            | 辅料       |

## 实施计划

### 技术选择

##### 拿取数据：

说明：爬取[美食杰](https://www.meishij.net/)，以下所作的分析的数据全部来自于美食杰网站

###### 技术选择：

requests库

xpath库分析

re库清洗

使用mysql和csv存放

##### 分析数据：

说明：使用sql分析出可视化要用的图表数据

###### 技术选择：

sql语句

##### 可视化数据：

说明：拿到分析到的数据使用echarts制成图表

###### 技术选择：

echarts:分析图表

jinja2：传输数据

pyecharts:[官网](https://pyecharts.org/)，将python和echarts结合一下，可以快速生成echarts图表

flask：使用flask框架生成

jieba：分词

wordcount:用来生成词云

##### 前端展现：

说明：通过html、css、js制作前端页面

###### 技术选择：

swiper:[官网](https://www.swiper.com.cn/)，生成轮播图，通过引入swiper.js文件编写配置即可使用

jquery:[官网](https://jquery.com/)，实现一些卡片功能

### 环境准备

开发工具：pycharm，vscode

大数据分析平台：sql

### 分析过程

###### 求出热门菜系TOP10

代码：

```sql
SELECT

dish_name,

private_int_fcount

FROM

foot_menu

ORDER BY

private_int_fcount DESC

LIMIT 0,

10;
```

结果：

![image-20210516190041439](https://i.loli.net/2021/05/16/LIryWYHDQSjTJRd.png)

###### 分析数据中各菜系的占比情况

代码：

```sql
SELECT DISTINCT

dishes_system,

COUNT( dishes_system ) AS count

FROM

foot_menu

GROUP BY

dishes_system

ORDER BY

count DESC;
```

效果：

![image-20210516190048441](https://i.loli.net/2021/05/16/jMX6pIAk1z4BiEb.png)

###### 3.分析主料和辅料的使用率

代码：

```sql
SELECT

main_ingredient

FROM

foot_menu;

SELECT

ingredient

FROM

foot_menu;
```

效果：

![image-20210516190059207](https://i.loli.net/2021/05/16/iIoYa6xjlMgvWzV.png)

### 前端借鉴：

**借鉴于**

**B站地址：https://space.bilibili.com/32683063?spm_id_from=333.788.b_765f7570696e666f.1**

**源码地址：https://github.com/local-host-8080/demo-html-css**

##### 首页：

导航栏：23-MenuEffect

卡片：31-Card-Effect

##### 百科：

卡片：45-FullCard

##### 展示：

图表展示：43-ProgressBar

### 可视化效果

![image-20210516190108004](https://i.loli.net/2021/05/16/l5YfRd4Q7nTp3ji.png)

![image-20210516190119123](https://i.loli.net/2021/05/16/zqCHc1PjS4pZrkQ.png)

![image-20210516190127058](https://i.loli.net/2021/05/16/opiJcZATXwBNGY7.png)

![image-20210516190137513](https://i.loli.net/2021/05/16/wZiQJNe6lzDP2Im.png)

## 结论与建议

1.  在美食杰网站上，通过热门菜系分析可以看出，前者多为肉类，相对于还是有许多人爱吃肉。

2.  前十菜系多为川菜，川菜喜欢的人多。

3.  通过各菜系占比情况可以看出江西菜、云南菜、贵州菜在同样爬取相同数量的情况之下还是居少数

4.  通过词云分析，主料一般使用五花肉、猪肉、鸡蛋、面粉、土豆居多，荤素、营养搭配居多

5.  辅料一般使用盐、料酒、生抽、老抽、白糖、姜、葱，可见盐是必不可少的。不过料酒烹调菜肴时不要放得过多，以免料酒味太重而影响菜肴本身的滋味。

# 谢谢指认其中错误，欢迎与我联系

[知乎地址](https://zhuanlan.zhihu.com/p/375496704)

[博客园地址](https://www.cnblogs.com/life-oss/p/14814628.html)